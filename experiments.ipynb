{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b423de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "import clinicadl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import log_loss\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# clinicaDL\n",
    "from clinicadl.tools.tsv.data_split import create_split\n",
    "from clinicadl.tools.deep_learning.data import generate_sampler, return_dataset, MRIDataset, MRIDatasetImage, MRIDatasetSlice, get_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from clinicadl.tools.deep_learning.cnn_utils import train, get_criterion, test\n",
    "from clinicadl.tools.deep_learning.models.random import RandomArchitecture\n",
    "from clinicadl.tools.deep_learning import EarlyStopping\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualization\n",
    "from scipy.ndimage import zoom\n",
    "import itkwidgets\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d32fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73f81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.explanations.evaluation import *\n",
    "from tools.explanations.GradCam import *\n",
    "from tools.explanations.guided_backprop import *\n",
    "from train.train_CNN import *\n",
    "from tools.callbacks import *\n",
    "from tools.data import *\n",
    "from tools.explanations.visualization import *\n",
    "from tools.models.CN5_FC3_3D import *\n",
    "from tools.settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29293b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021.tsv'\n",
    "summary_path = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021_summary.tsv'\n",
    "# df_data = pd.read_csv(data_path,sep='\\t',nrows=10)\n",
    "# df_summary = pd.read_csv(summary_path,sep='\\t',nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb619243",
   "metadata": {},
   "source": [
    "# Train Single CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e3401",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67523b0",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "caps_directory = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021/'\n",
    "batch_size = 4\n",
    "num_workers = os.cpu_count()\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efac0b65",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:p_age=0.28, p_sex=0.8008\n",
      "DEBUG:root:p_age=0.77, p_sex=0.9217\n",
      "DEBUG:root:p_age=0.76, p_sex=0.9114\n",
      "DEBUG:root:p_age=0.00, p_sex=0.9951\n",
      "DEBUG:root:p_age=0.46, p_sex=0.8941\n",
      "DEBUG:root:p_age=0.39, p_sex=0.7858\n",
      "DEBUG:root:p_age=0.37, p_sex=0.8395\n",
      "DEBUG:root:p_age=0.58, p_sex=0.9393\n",
      "DEBUG:root:p_age=0.48, p_sex=0.9951\n",
      "DEBUG:root:p_age=0.13, p_sex=0.9493\n",
      "DEBUG:root:p_age=0.07, p_sex=0.8125\n",
      "DEBUG:root:p_age=0.30, p_sex=0.9393\n",
      "DEBUG:root:p_age=0.84, p_sex=0.9771\n",
      "INFO:root:Split for diagnosis AD was found after 13 trials\n",
      "DEBUG:root:p_age=0.99, p_sex=0.9954\n",
      "INFO:root:Split for diagnosis CN was found after 1 trials\n"
     ]
    }
   ],
   "source": [
    "# load dataframes\n",
    "AD = pd.read_csv('subjects/AD.tsv',sep='\\t')\n",
    "CN = pd.read_csv('subjects/CN.tsv',sep='\\t')\n",
    "\n",
    "# remove samples with NaN\n",
    "AD.drop(AD[AD.isna().sum(axis=1) > 0].index, inplace=True)\n",
    "CN.drop(CN[CN.isna().sum(axis=1) > 0].index, inplace=True)\n",
    "\n",
    "# split data between training and validation sets\n",
    "training_df, valid_df = create_split('AD', AD, 'diagnosis',0.2)\n",
    "df_CN = create_split('CN', CN, 'diagnosis',0.2)\n",
    "training_df = training_df.append(df_CN[0]).reset_index().iloc[np.array([0,1,2,-1,-2,-3])]\n",
    "valid_df = valid_df.append(df_CN[1]).reset_index().iloc[np.array([0,1,2,-1,-2,-3])]\n",
    "\n",
    "# drop index column\n",
    "training_df.drop(columns = ['index'], inplace=True)\n",
    "valid_df.drop(columns = ['index'], inplace=True)\n",
    "\n",
    "train_transforms, all_transforms = get_transforms('image', minmaxnormalization=True, data_augmentation=None )\n",
    "# fetch volumetric data\n",
    "stds, df_add_data = fetch_add_data(training_df)\n",
    "\n",
    "# all_transforms = torchvision.transforms.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa898c26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_train = MRIDatasetImage(caps_directory, training_df, df_add_data=df_add_data,all_transformations=all_transforms) #train_transformations=all_transforms\n",
    "data_valid = MRIDatasetImage(caps_directory, valid_df, df_add_data=df_add_data, all_transformations=all_transforms) #train_transformations=all_transforms,\n",
    "\n",
    "\n",
    "# sampler\n",
    "train_sampler = generate_sampler(data_train)\n",
    "valid_sampler = generate_sampler(data_valid)\n",
    "# loaders\n",
    "train_loader = DataLoader(data_train,\n",
    "                         batch_size=batch_size,\n",
    "                         sampler=train_sampler,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=True)\n",
    "\n",
    "valid_loader = DataLoader(data_valid,\n",
    "                         batch_size=batch_size,\n",
    "                         sampler=valid_sampler,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9be74e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b66241",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      --                        --\n",
      "├─Sequential: 1-1                        [4, 64, 8, 11, 9]         --\n",
      "│    └─Conv3d: 2-1                       [4, 8, 167, 206, 177]     224\n",
      "│    └─ReLU: 2-2                         [4, 8, 167, 206, 177]     --\n",
      "│    └─BatchNorm3d: 2-3                  [4, 8, 167, 206, 177]     16\n",
      "│    └─MaxPool3d: 2-4                    [4, 8, 83, 103, 88]       --\n",
      "│    └─Conv3d: 2-5                       [4, 16, 81, 101, 86]      3,472\n",
      "│    └─ReLU: 2-6                         [4, 16, 81, 101, 86]      --\n",
      "│    └─BatchNorm3d: 2-7                  [4, 16, 81, 101, 86]      32\n",
      "│    └─MaxPool3d: 2-8                    [4, 16, 40, 50, 43]       --\n",
      "│    └─Conv3d: 2-9                       [4, 32, 38, 48, 41]       13,856\n",
      "│    └─ReLU: 2-10                        [4, 32, 38, 48, 41]       --\n",
      "│    └─BatchNorm3d: 2-11                 [4, 32, 38, 48, 41]       64\n",
      "│    └─MaxPool3d: 2-12                   [4, 32, 19, 24, 20]       --\n",
      "│    └─Conv3d: 2-13                      [4, 64, 17, 22, 18]       55,360\n",
      "│    └─ReLU: 2-14                        [4, 64, 17, 22, 18]       --\n",
      "│    └─BatchNorm3d: 2-15                 [4, 64, 17, 22, 18]       128\n",
      "│    └─MaxPool3d: 2-16                   [4, 64, 8, 11, 9]         --\n",
      "├─Sequential: 1-2                        [4, 1]                    --\n",
      "│    └─Conv3d: 2-17                      [4, 128, 6, 9, 7]         221,312\n",
      "│    └─ReLU: 2-18                        [4, 128, 6, 9, 7]         --\n",
      "│    └─BatchNorm3d: 2-19                 [4, 128, 6, 9, 7]         256\n",
      "│    └─MaxPool3d: 2-20                   [4, 128, 3, 4, 3]         --\n",
      "│    └─Flatten: 2-21                     [4, 4608]                 --\n",
      "│    └─Linear: 2-22                      [4, 32]                   147,488\n",
      "│    └─ReLU: 2-23                        [4, 32]                   --\n",
      "│    └─BatchNorm1d: 2-24                 [4, 32]                   64\n",
      "│    └─Dropout: 2-25                     [4, 32]                   --\n",
      "│    └─Linear: 2-26                      [4, 16]                   528\n",
      "│    └─ReLU: 2-27                        [4, 16]                   --\n",
      "│    └─BatchNorm1d: 2-28                 [4, 16]                   32\n",
      "│    └─Dropout: 2-29                     [4, 16]                   --\n",
      "│    └─Linear: 2-30                      [4, 1]                    17\n",
      "│    └─Sigmoid: 2-31                     [4, 1]                    --\n",
      "├─Sequential: 1-3                        [4, 119]                  --\n",
      "│    └─Conv3d: 2-32                      [4, 128, 6, 9, 7]         221,312\n",
      "│    └─ReLU: 2-33                        [4, 128, 6, 9, 7]         --\n",
      "│    └─BatchNorm3d: 2-34                 [4, 128, 6, 9, 7]         256\n",
      "│    └─MaxPool3d: 2-35                   [4, 128, 3, 4, 3]         --\n",
      "│    └─Flatten: 2-36                     [4, 4608]                 --\n",
      "│    └─Linear: 2-37                      [4, 476]                  2,193,884\n",
      "│    └─ReLU: 2-38                        [4, 476]                  --\n",
      "│    └─BatchNorm1d: 2-39                 [4, 476]                  952\n",
      "│    └─Dropout: 2-40                     [4, 476]                  --\n",
      "│    └─Linear: 2-41                      [4, 238]                  113,526\n",
      "│    └─ReLU: 2-42                        [4, 238]                  --\n",
      "│    └─BatchNorm1d: 2-43                 [4, 238]                  476\n",
      "│    └─Dropout: 2-44                     [4, 238]                  --\n",
      "│    └─Linear: 2-45                      [4, 119]                  28,441\n",
      "├─Sequential: 1-4                        [4, 1]                    --\n",
      "│    └─Conv3d: 2-46                      [4, 128, 6, 9, 7]         221,312\n",
      "│    └─ReLU: 2-47                        [4, 128, 6, 9, 7]         --\n",
      "│    └─BatchNorm3d: 2-48                 [4, 128, 6, 9, 7]         256\n",
      "│    └─MaxPool3d: 2-49                   [4, 128, 3, 4, 3]         --\n",
      "│    └─Flatten: 2-50                     [4, 4608]                 --\n",
      "│    └─Linear: 2-51                      [4, 32]                   147,488\n",
      "│    └─ReLU: 2-52                        [4, 32]                   --\n",
      "│    └─BatchNorm1d: 2-53                 [4, 32]                   64\n",
      "│    └─Dropout: 2-54                     [4, 32]                   --\n",
      "│    └─Linear: 2-55                      [4, 16]                   528\n",
      "│    └─ReLU: 2-56                        [4, 16]                   --\n",
      "│    └─BatchNorm1d: 2-57                 [4, 16]                   32\n",
      "│    └─Dropout: 2-58                     [4, 16]                   --\n",
      "│    └─Linear: 2-59                      [4, 1]                    17\n",
      "├─Sequential: 1-5                        [4, 1]                    --\n",
      "│    └─Conv3d: 2-60                      [4, 128, 6, 9, 7]         221,312\n",
      "│    └─ReLU: 2-61                        [4, 128, 6, 9, 7]         --\n",
      "│    └─BatchNorm3d: 2-62                 [4, 128, 6, 9, 7]         256\n",
      "│    └─MaxPool3d: 2-63                   [4, 128, 3, 4, 3]         --\n",
      "│    └─Flatten: 2-64                     [4, 4608]                 --\n",
      "│    └─Linear: 2-65                      [4, 32]                   147,488\n",
      "│    └─ReLU: 2-66                        [4, 32]                   --\n",
      "│    └─BatchNorm1d: 2-67                 [4, 32]                   64\n",
      "│    └─Dropout: 2-68                     [4, 32]                   --\n",
      "│    └─Linear: 2-69                      [4, 16]                   528\n",
      "│    └─ReLU: 2-70                        [4, 16]                   --\n",
      "│    └─BatchNorm1d: 2-71                 [4, 16]                   32\n",
      "│    └─Dropout: 2-72                     [4, 16]                   --\n",
      "│    └─Linear: 2-73                      [4, 1]                    17\n",
      "│    └─Sigmoid: 2-74                     [4, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 3,741,090\n",
      "Trainable params: 3,741,090\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 22.21\n",
      "==========================================================================================\n",
      "Input size (MB): 100.68\n",
      "Forward/backward pass size (MB): 4031.28\n",
      "Params size (MB): 14.96\n",
      "Estimated Total Size (MB): 4146.92\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# get sample\n",
    "sample = data_train[0]\n",
    "# build model\n",
    "model = Net(sample, [8, 16, 32, 64, 128])\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"To cuda\")\n",
    "#     model.cuda()\n",
    "model.summary(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27816173",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of the training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1625, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1632, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/tools/data.py\", line 201, in __getitem__\n    participant, session, cohort, _, label = self._get_meta_data(idx)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/clinicadl/tools/deep_learning/data.py\", line 140, in _get_meta_data\n    participant = self.df.loc[image_idx, 'participant_id']\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 889, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1060, in _getitem_tuple\n    return self._getitem_lowerdim(tup)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 807, in _getitem_lowerdim\n    section = self._getitem_axis(key, axis=i)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1124, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1073, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/generic.py\", line 3739, in xs\n    loc = index.get_loc(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3750868908c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/train/train_CNN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer_, loader, loss_weights, to_cuda, rescaling)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1625, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1632, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/tools/data.py\", line 201, in __getitem__\n    participant, session, cohort, _, label = self._get_meta_data(idx)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/clinicadl/tools/deep_learning/data.py\", line 140, in _get_meta_data\n    participant = self.df.loc[image_idx, 'participant_id']\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 889, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1060, in _getitem_tuple\n    return self._getitem_lowerdim(tup)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 807, in _getitem_lowerdim\n    section = self._getitem_axis(key, axis=i)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1124, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1073, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/generic.py\", line 3739, in xs\n    loc = index.get_loc(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# record losses\n",
    "train_losses = dict()\n",
    "test_losses = dict()\n",
    "\n",
    "# callbacks\n",
    "ES = EarlyStopping(patience=5)\n",
    "MC = ModelCheckpoint()\n",
    "\n",
    "print(\"Beginning of the training\")\n",
    "\n",
    "# training\n",
    "for epoch in range(2):\n",
    "    update_dict(train_losses, train(epoch, model, optimizer, train_loader, to_cuda=False))\n",
    "    update_dict(test_losses, test(model, valid_loader, to_cuda=False, rescaling=stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ff3f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for k in range(1,5):\n",
    "    print(np.linalg.norm(getattr(getattr(model,'branch' + str(k)), 'b' + str(k) + '-conv').weight.grad.data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c3703",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08832b",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(dict_losses, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the different losses.\n",
    "    \n",
    "    Args:\n",
    "        dict_losses: dictionnary of losses\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    for key in dict_losses.keys():\n",
    "        plt.plot(dict_losses[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tensor(X):\n",
    "    x = np.transpose(X[0], (1,2,0))\n",
    "    x = (x-x.min())/x.max()\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb76b5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metric_path = 'results/models/model_5/'\n",
    "\n",
    "train_metrics = pd.read_csv(metric_path + 'train_losses.csv')\n",
    "test_metrics = pd.read_csv(metric_path + 'val_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d51b84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_losses(train_metrics[['disease']], \"Training losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f6ecb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_losses(test_metrics[['disease']], \"Val losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cce37e",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93691a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_directory = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021/'\n",
    "path = 'results/models/model_19/'\n",
    "training_df = pd.read_csv(os.path.join(path, 'training_df.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(path, 'valid_df.csv'))\n",
    "\n",
    "train_transforms, all_transforms = get_transforms('image', minmaxnormalization=True, data_augmentation=None)\n",
    "\n",
    "# fetch volumetric data\n",
    "stds, df_add_data = fetch_add_data(training_df)\n",
    "\n",
    "# all_transforms = torchvision.transforms.Compose([])\n",
    "\n",
    "data_train = MRIDatasetImage(caps_directory, training_df, df_add_data=df_add_data,\n",
    "                             all_transformations=all_transforms)  # train_transformations=all_transforms\n",
    "data_valid = MRIDatasetImage(caps_directory, valid_df, df_add_data=df_add_data,\n",
    "                             all_transformations=all_transforms)  # train_transformations=all_transforms,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93e4bfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb37b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "saved_data = torch.load(os.path.join(path, 'test_best_model.pt'))\n",
    "\n",
    "model.load_state_dict(saved_data['model_state_dict'])\n",
    "\n",
    "# select one image\n",
    "img = sample['image'].float()\n",
    "\n",
    "mode = 'cuda'\n",
    "if mode == 'cuda':\n",
    "    model = model.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    model = model.cpu()\n",
    "    img = img.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea83d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = np.load('results/models/model_19/attribution_maps/GC/val/age/sub-ADNI002S1018_ses-M00.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bda604",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9a8726",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "saved_data = torch.load('results/models/model_19/last_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba39a1f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'best_loss', 'train_metrics', 'val_metrics'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a47e586",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_metrics = saved_data['val_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1124cbad",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.740900039672852,\n",
       " 8.37081527709961,\n",
       " 33.91691589355469,\n",
       " 82.28373718261719,\n",
       " 28.77964973449707,\n",
       " 21.143688201904297,\n",
       " 16.262182235717773,\n",
       " 19.61556625366211,\n",
       " 10.508869171142578,\n",
       " 8.14531135559082,\n",
       " 91.03712463378906,\n",
       " 51.513816833496094,\n",
       " 107.070556640625,\n",
       " 123.70819854736328,\n",
       " 21.909709930419922,\n",
       " 51.294586181640625,\n",
       " 6.470217704772949,\n",
       " 31.632925033569336,\n",
       " 6.6416826248168945,\n",
       " 6.601271629333496,\n",
       " 6.524799346923828,\n",
       " 6.593179702758789,\n",
       " 6.520787715911865,\n",
       " 15.977116584777832,\n",
       " 6.48101806640625,\n",
       " 6.303374290466309,\n",
       " 6.631115913391113,\n",
       " 6.688444137573242,\n",
       " 6.196437835693359,\n",
       " 6.953071594238281,\n",
       " 16.680301666259766,\n",
       " 6.913064956665039,\n",
       " 6.533334732055664,\n",
       " 43.05387878417969,\n",
       " 11.138813972473145,\n",
       " 6.076729774475098,\n",
       " 6.3126044273376465,\n",
       " 36.24996566772461,\n",
       " 52.37376403808594,\n",
       " 28.881973266601562,\n",
       " 5.843369483947754,\n",
       " 5.774815082550049,\n",
       " 5.958796501159668,\n",
       " 5.991237640380859,\n",
       " 13.440272331237793,\n",
       " 5.844188213348389,\n",
       " 6.305954456329346,\n",
       " 5.875237464904785,\n",
       " 5.777097225189209,\n",
       " 5.758735179901123,\n",
       " 5.689347743988037,\n",
       " 5.864001750946045,\n",
       " 5.804633140563965,\n",
       " 5.816695213317871,\n",
       " 6.000572681427002,\n",
       " 5.6244072914123535,\n",
       " 5.511514663696289,\n",
       " 5.761058807373047,\n",
       " 5.776197910308838,\n",
       " 5.7831950187683105,\n",
       " 5.536807060241699,\n",
       " 5.893636703491211,\n",
       " 22.49083137512207,\n",
       " 5.834686756134033,\n",
       " 5.521534442901611]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['b3_MeanSquaredError']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd877ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(train_metrics)['b3_MeanSquaredError'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7496f0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb3ab1",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GBP = GuidedBackprop(model)\n",
    "attention_maps = GBP.generate_gradients(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55feea14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize_explanations(img.cpu(), attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbf4a1",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "453f2de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.476941347122192\n"
     ]
    }
   ],
   "source": [
    "a0 = time.time()\n",
    "GC = GradCam(model)\n",
    "img = sample['image']\n",
    "attentions = GC.get_explanations(img.cuda(), resize=False, to_cpu=True)\n",
    "print(time.time() - a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c11aca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: 169 208 179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bcbbf78e4e4575b65dcee42c155b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=103, description='layer', max=207), IntSlider(value=0, description='chan…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for key in attentions:\n",
    "#     attentions[key] = attentions[key].cpu()\n",
    "visualize_explanations(img, attentions) #, targets='volumes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b69953",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = time.time()\n",
    "GC = GradCam(model.cuda(), target_layer='conv2')\n",
    "img = sample['image'].float()\n",
    "attentions = GC.get_explanations(img.cuda(), resize=True, to_cpu=True)\n",
    "print(time.time() - a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca37236",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_explanations(img, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab791723",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_explanations(img, attentions, targets='disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006ddc6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### max-sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62552349",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d = [], [], [], []\n",
    "n_max = 500\n",
    "step = 20\n",
    "for k in tqdm(range(1, n_max, step)):\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    resu = max_sensitivity(img, GC, k)\n",
    "    a.append(resu['branch1'])\n",
    "    b.append(resu['branch2'])\n",
    "    c.append(resu['branch3'])\n",
    "    d.append(resu['branch4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca86141",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "max_sensitivity(img, GC, N=1)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed76d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for elem in (a, b, c ,d):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1,n_max,step), elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7be98",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MoRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d3410",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attention = attentions['branch1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8e733",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ids = np.unravel_index(np.argsort(-attention, axis=None), attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c76b26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "group_size = 20000\n",
    "batch_X = torch.tile(img[None,...], (batch_size,1,1,1,1))\n",
    "for k in range(1, batch_size):\n",
    "    index = k*group_size\n",
    "    batch_X[k,0,ids[0][:index], ids[1][:index], ids[2][:index]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715326f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2 = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e5b58",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resu = model2(batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a2c89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = sample['image'].float()\n",
    "np.prod(img.shape)//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde01433",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MoRF(X, model, exp_method, K=None, group_size=20000, AUC=False, batch_size=16, to_cuda=False):\n",
    "    \"\"\"\n",
    "    Most relevant first: measures the reliability of an explanation by testing \n",
    "    how fast the output decreases, while we progressively remove information (e.g., perturb pixels) \n",
    "    from the input 𝑥𝑥 (e.g., image), that appears as the most relevant by the explanation.\n",
    "    Args:\n",
    "        X: tensor, brain image, with shape (1, n_channels, depth, height, width). The two first dimensions\n",
    "            are optional.\n",
    "        exp_method: explanation method. Must have a get_explanations(self, input_image) attribute function\n",
    "            which takes an image as input and returns a dictionary mapping branches to explanation maps\n",
    "        K: number of group of relevant pixels to remove\n",
    "        group_size: int, size of a group of pixels to remove\n",
    "        AUC: bool. If True: compute and return area under the curve obtained after removing successively \n",
    "            the K most relevant pixels.\n",
    "        batch_size: int, number of images passed to the model each time\n",
    "            \n",
    "    TO DO:\n",
    "        - add several methods to perturb pixels\n",
    "    \"\"\"\n",
    "    if to_cuda and torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        \n",
    "    if K is None:\n",
    "        K = np.prod(X.shape)//8\n",
    "    \n",
    "    # reshpae X if necessary\n",
    "    while len(X.shape) < 5:\n",
    "        X = X[None,...]\n",
    "    \n",
    "    # original predictions\n",
    "    preds = model(X)\n",
    "    # explanations for original image\n",
    "    expls = exp_method.get_explanations(X, resize=True)\n",
    "    # explanations for new images\n",
    "    new_preds = dict()\n",
    "    \n",
    "    # def update_dict\n",
    "    \n",
    "    for target in expls:\n",
    "        # Indices of the sorted elements of the explanations:\n",
    "        ids = np.unravel_index(np.argsort(-expls[target], axis=None), expls[target].shape)\n",
    "    \n",
    "        if AUC:\n",
    "            # number of \n",
    "            removed_pixels = 0\n",
    "            while removed_pixels < K:\n",
    "                # create batch of images\n",
    "                bs = min(batch_size,(K - removed_pixels)%group_size)\n",
    "                batch_X = torch.tile(X, (bs,1,1,1,1))\n",
    "                for k in range(1, batch_size):\n",
    "                    index = k*group_size\n",
    "                    batch_X[k,0,ids[0][:index], ids[1][:index], ids[2][:index]] = 0\n",
    "                    new_preds[target] = model(batch_X)\n",
    "        else:\n",
    "            # compute MoRF removing the K most relevant pixels\n",
    "            batch_X = X.copy()\n",
    "            batch_X[0,0,ids[0][:K], ids[1][:K], ids[2][:K]]\n",
    "            new_preds[target] = model(batch_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e28b2",
   "metadata": {},
   "source": [
    "### Explanations analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed2ca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu peux en trouver dans le code source de clinica clinica/resources/atlases\n",
    "# Et tu peux prendre par exemple atlas-AAL2_dseg.nii.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72cfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average map\n",
    "path = '/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/results/models/model_19/attribution_maps/GC/val/age'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ab3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(os.path.join(path, files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8943344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: 6 9 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dd58994f704c1c81c52181193446e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='layer', max=8), IntSlider(value=0, description='channel'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_explanations(torch.tensor(arr),{'branch3': arr},targets='age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89dc28",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a1bc3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = pd.read_csv('train_losses_3D_2.csv')\n",
    "test_losses = pd.read_csv('val_losses_3D_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65649095",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f319ae7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(test_losses[['disease', 'volumes', 'age', 'sex']])\n",
    "plt.ylim([0,150])\n",
    "plt.legend(['disease', 'volumes', 'age', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991ce04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[['disease', 'volumes', 'age', 'sex']])\n",
    "plt.legend(['disease', 'volumes', 'age', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09078e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall\n",
    "target = torch.tensor([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "preds = torch.tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
    "metrics = MetricCollection([Accuracy(),\n",
    "                            Precision(num_classes=2, average='micro'),\n",
    "                            Recall(num_classes=3, average='macro')])\n",
    "metrics(preds, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d00a1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# assess gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4813001",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_gradient_norms(model): \n",
    "    path = 'results/models/model_{}/log.out'.format(model)\n",
    "    \n",
    "    file = None\n",
    "    with open(path, 'r') as f:\n",
    "        file = f.read()\n",
    "\n",
    "    data = [ s[36:] for s in file.split('\\n') if 'STDOUT' in s]\n",
    "    data = [ s for k,s in enumerate(data[1:]) if data[k] == 'GRADIENT']\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "def compute_metrics(data, name):\n",
    "    dict_ = {'mean': data.mean(),\n",
    "             'min': data.min(),\n",
    "             'max': data.max(), \n",
    "             'std': data.std(), \n",
    "             'median': np.median(data)}\n",
    "    return pd.DataFrame(dict_, index=[name])\n",
    "\n",
    "def get_all_metrics():\n",
    "    df = None\n",
    "    for k in range(4):\n",
    "        data = get_gradient_norms(9+k)\n",
    "        if df is None:\n",
    "            df = compute_metrics(data,BRANCH2TARGET['branch' + str(k+1)])\n",
    "        else:\n",
    "            df = df.combine_first(compute_metrics(data,BRANCH2TARGET['branch' + str(k+1)]))\n",
    "    return df\n",
    "\n",
    "def plot_training(model):\n",
    "    fig, ax = plt.subplots(1,4,figsize=(16,4))\n",
    "    for k, target in enumerate(TARGET2BRANCH):\n",
    "        df_train = pd.read_csv('results/models/model_{}/train_losses.csv'.format(model))\n",
    "        df_val = pd.read_csv('results/models/model_{}/val_losses.csv'.format(model))\n",
    "        ax[k].plot(getattr(df_train, target), label='train')\n",
    "        ax[k].plot(getattr(df_val, target), label='val')\n",
    "        ax[k].set_title(target)\n",
    "    plt.legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09e46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = get_all_metrics()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707fc1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights = (1/df['mean']).to_numpy()\n",
    "weights = weights/weights.sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4639e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_training(model=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996e6bb",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('results/models/model_{}/val_losses.csv'.format(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9f97d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Age investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f54fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([training_df, valid_df], ignore_index=True)\n",
    "df2 = df_add_data.merge(df, on =['participant_id', 'session_id'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c7107",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.title('Age Distribution')\n",
    "plt.hist(training_df.age, label='training')\n",
    "plt.hist(valid_df.age, label='validation')\n",
    "plt.xlabel('age')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99854d6c",
   "metadata": {},
   "source": [
    "# RANDOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cbb3a",
   "metadata": {},
   "source": [
    "I am using working with the file '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021.tsv' and using the 't1-volume' pipeline and the atlas 'AAL2'.\n",
    "6267 samples (i.e. couples (participant_id, session_id)) do not have any volume value (i.e. NaN). 1 sample has no sex value and 1178 samples have no age values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
