{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089f6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b423de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "import clinicadl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import log_loss\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# clinicaDL\n",
    "from clinicadl.tools.tsv.data_split import create_split\n",
    "from clinicadl.tools.deep_learning.data import generate_sampler, return_dataset, MRIDataset, MRIDatasetImage, MRIDatasetSlice, get_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from clinicadl.tools.deep_learning.cnn_utils import train, get_criterion, test\n",
    "from clinicadl.tools.deep_learning.models.random import RandomArchitecture\n",
    "from clinicadl.tools.deep_learning import EarlyStopping\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualization\n",
    "from scipy.ndimage import zoom\n",
    "import itkwidgets\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d32fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73f81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.explanations.evaluation import *\n",
    "from tools.explanations.GradCam import *\n",
    "from tools.explanations.guided_backprop import *\n",
    "from train.train_CNN import *\n",
    "from tools.callbacks import *\n",
    "from tools.data import *\n",
    "from tools.explanations.visualization import *\n",
    "from tools.models.CN5_FC3_3D import *\n",
    "from tools.settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29293b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021.tsv'\n",
    "summary_path = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021_summary.tsv'\n",
    "# df_data = pd.read_csv(data_path,sep='\\t',nrows=10)\n",
    "# df_summary = pd.read_csv(summary_path,sep='\\t',nrows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb619243",
   "metadata": {},
   "source": [
    "# Train Single CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e3401",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67523b0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# global parameters\n",
    "caps_directory = '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021/'\n",
    "batch_size = 4\n",
    "num_workers = os.cpu_count()\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efac0b65",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:p_age=0.28, p_sex=0.8008\n",
      "DEBUG:root:p_age=0.77, p_sex=0.9217\n",
      "DEBUG:root:p_age=0.76, p_sex=0.9114\n",
      "DEBUG:root:p_age=0.00, p_sex=0.9951\n",
      "DEBUG:root:p_age=0.46, p_sex=0.8941\n",
      "DEBUG:root:p_age=0.39, p_sex=0.7858\n",
      "DEBUG:root:p_age=0.37, p_sex=0.8395\n",
      "DEBUG:root:p_age=0.58, p_sex=0.9393\n",
      "DEBUG:root:p_age=0.48, p_sex=0.9951\n",
      "DEBUG:root:p_age=0.13, p_sex=0.9493\n",
      "DEBUG:root:p_age=0.07, p_sex=0.8125\n",
      "DEBUG:root:p_age=0.30, p_sex=0.9393\n",
      "DEBUG:root:p_age=0.84, p_sex=0.9771\n",
      "INFO:root:Split for diagnosis AD was found after 13 trials\n",
      "DEBUG:root:p_age=0.99, p_sex=0.9954\n",
      "INFO:root:Split for diagnosis CN was found after 1 trials\n"
     ]
    }
   ],
   "source": [
    "# load dataframes\n",
    "AD = pd.read_csv('subjects/AD.tsv',sep='\\t')\n",
    "CN = pd.read_csv('subjects/CN.tsv',sep='\\t')\n",
    "\n",
    "# remove samples with NaN\n",
    "AD.drop(AD[AD.isna().sum(axis=1) > 0].index, inplace=True)\n",
    "CN.drop(CN[CN.isna().sum(axis=1) > 0].index, inplace=True)\n",
    "\n",
    "# split data between training and validation sets\n",
    "training_df, valid_df = create_split('AD', AD, 'diagnosis',0.2)\n",
    "df_CN = create_split('CN', CN, 'diagnosis',0.2)\n",
    "training_df = training_df.append(df_CN[0]).reset_index().iloc[np.array([0,1,2,-1,-2,-3])]\n",
    "valid_df = valid_df.append(df_CN[1]).reset_index().iloc[np.array([0,1,2,-1,-2,-3])]\n",
    "\n",
    "# drop index column\n",
    "training_df.drop(columns = ['index'], inplace=True)\n",
    "valid_df.drop(columns = ['index'], inplace=True)\n",
    "\n",
    "train_transforms, all_transforms = get_transforms('image', minmaxnormalization=True, data_augmentation=None )\n",
    "# fetch volumetric data\n",
    "stds, df_add_data = fetch_add_data(training_df)\n",
    "\n",
    "# all_transforms = torchvision.transforms.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa898c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = MRIDatasetImage(caps_directory, training_df, df_add_data=df_add_data,all_transformations=all_transforms) #train_transformations=all_transforms\n",
    "data_valid = MRIDatasetImage(caps_directory, valid_df, df_add_data=df_add_data, all_transformations=all_transforms) #train_transformations=all_transforms,\n",
    "\n",
    "\n",
    "# sampler\n",
    "train_sampler = generate_sampler(data_train)\n",
    "valid_sampler = generate_sampler(data_valid)\n",
    "# loaders\n",
    "train_loader = DataLoader(data_train,\n",
    "                         batch_size=batch_size,\n",
    "                         sampler=train_sampler,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=True)\n",
    "\n",
    "valid_loader = DataLoader(data_valid,\n",
    "                         batch_size=batch_size,\n",
    "                         sampler=valid_sampler,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9be74e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b66241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      --                        --\n",
      "â”œâ”€Sequential: 1-1                        [4, 64, 8, 11, 9]         --\n",
      "â”‚    â””â”€Conv3d: 2-1                       [4, 8, 167, 206, 177]     224\n",
      "â”‚    â””â”€ReLU: 2-2                         [4, 8, 167, 206, 177]     --\n",
      "â”‚    â””â”€BatchNorm3d: 2-3                  [4, 8, 167, 206, 177]     16\n",
      "â”‚    â””â”€MaxPool3d: 2-4                    [4, 8, 83, 103, 88]       --\n",
      "â”‚    â””â”€Conv3d: 2-5                       [4, 16, 81, 101, 86]      3,472\n",
      "â”‚    â””â”€ReLU: 2-6                         [4, 16, 81, 101, 86]      --\n",
      "â”‚    â””â”€BatchNorm3d: 2-7                  [4, 16, 81, 101, 86]      32\n",
      "â”‚    â””â”€MaxPool3d: 2-8                    [4, 16, 40, 50, 43]       --\n",
      "â”‚    â””â”€Conv3d: 2-9                       [4, 32, 38, 48, 41]       13,856\n",
      "â”‚    â””â”€ReLU: 2-10                        [4, 32, 38, 48, 41]       --\n",
      "â”‚    â””â”€BatchNorm3d: 2-11                 [4, 32, 38, 48, 41]       64\n",
      "â”‚    â””â”€MaxPool3d: 2-12                   [4, 32, 19, 24, 20]       --\n",
      "â”‚    â””â”€Conv3d: 2-13                      [4, 64, 17, 22, 18]       55,360\n",
      "â”‚    â””â”€ReLU: 2-14                        [4, 64, 17, 22, 18]       --\n",
      "â”‚    â””â”€BatchNorm3d: 2-15                 [4, 64, 17, 22, 18]       128\n",
      "â”‚    â””â”€MaxPool3d: 2-16                   [4, 64, 8, 11, 9]         --\n",
      "â”œâ”€Sequential: 1-2                        [4, 1]                    --\n",
      "â”‚    â””â”€Conv3d: 2-17                      [4, 128, 6, 9, 7]         221,312\n",
      "â”‚    â””â”€ReLU: 2-18                        [4, 128, 6, 9, 7]         --\n",
      "â”‚    â””â”€BatchNorm3d: 2-19                 [4, 128, 6, 9, 7]         256\n",
      "â”‚    â””â”€MaxPool3d: 2-20                   [4, 128, 3, 4, 3]         --\n",
      "â”‚    â””â”€Flatten: 2-21                     [4, 4608]                 --\n",
      "â”‚    â””â”€Linear: 2-22                      [4, 32]                   147,488\n",
      "â”‚    â””â”€ReLU: 2-23                        [4, 32]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-24                 [4, 32]                   64\n",
      "â”‚    â””â”€Dropout: 2-25                     [4, 32]                   --\n",
      "â”‚    â””â”€Linear: 2-26                      [4, 16]                   528\n",
      "â”‚    â””â”€ReLU: 2-27                        [4, 16]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-28                 [4, 16]                   32\n",
      "â”‚    â””â”€Dropout: 2-29                     [4, 16]                   --\n",
      "â”‚    â””â”€Linear: 2-30                      [4, 1]                    17\n",
      "â”‚    â””â”€Sigmoid: 2-31                     [4, 1]                    --\n",
      "â”œâ”€Sequential: 1-3                        [4, 119]                  --\n",
      "â”‚    â””â”€Conv3d: 2-32                      [4, 128, 6, 9, 7]         221,312\n",
      "â”‚    â””â”€ReLU: 2-33                        [4, 128, 6, 9, 7]         --\n",
      "â”‚    â””â”€BatchNorm3d: 2-34                 [4, 128, 6, 9, 7]         256\n",
      "â”‚    â””â”€MaxPool3d: 2-35                   [4, 128, 3, 4, 3]         --\n",
      "â”‚    â””â”€Flatten: 2-36                     [4, 4608]                 --\n",
      "â”‚    â””â”€Linear: 2-37                      [4, 476]                  2,193,884\n",
      "â”‚    â””â”€ReLU: 2-38                        [4, 476]                  --\n",
      "â”‚    â””â”€BatchNorm1d: 2-39                 [4, 476]                  952\n",
      "â”‚    â””â”€Dropout: 2-40                     [4, 476]                  --\n",
      "â”‚    â””â”€Linear: 2-41                      [4, 238]                  113,526\n",
      "â”‚    â””â”€ReLU: 2-42                        [4, 238]                  --\n",
      "â”‚    â””â”€BatchNorm1d: 2-43                 [4, 238]                  476\n",
      "â”‚    â””â”€Dropout: 2-44                     [4, 238]                  --\n",
      "â”‚    â””â”€Linear: 2-45                      [4, 119]                  28,441\n",
      "â”œâ”€Sequential: 1-4                        [4, 1]                    --\n",
      "â”‚    â””â”€Conv3d: 2-46                      [4, 128, 6, 9, 7]         221,312\n",
      "â”‚    â””â”€ReLU: 2-47                        [4, 128, 6, 9, 7]         --\n",
      "â”‚    â””â”€BatchNorm3d: 2-48                 [4, 128, 6, 9, 7]         256\n",
      "â”‚    â””â”€MaxPool3d: 2-49                   [4, 128, 3, 4, 3]         --\n",
      "â”‚    â””â”€Flatten: 2-50                     [4, 4608]                 --\n",
      "â”‚    â””â”€Linear: 2-51                      [4, 32]                   147,488\n",
      "â”‚    â””â”€ReLU: 2-52                        [4, 32]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-53                 [4, 32]                   64\n",
      "â”‚    â””â”€Dropout: 2-54                     [4, 32]                   --\n",
      "â”‚    â””â”€Linear: 2-55                      [4, 16]                   528\n",
      "â”‚    â””â”€ReLU: 2-56                        [4, 16]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-57                 [4, 16]                   32\n",
      "â”‚    â””â”€Dropout: 2-58                     [4, 16]                   --\n",
      "â”‚    â””â”€Linear: 2-59                      [4, 1]                    17\n",
      "â”œâ”€Sequential: 1-5                        [4, 1]                    --\n",
      "â”‚    â””â”€Conv3d: 2-60                      [4, 128, 6, 9, 7]         221,312\n",
      "â”‚    â””â”€ReLU: 2-61                        [4, 128, 6, 9, 7]         --\n",
      "â”‚    â””â”€BatchNorm3d: 2-62                 [4, 128, 6, 9, 7]         256\n",
      "â”‚    â””â”€MaxPool3d: 2-63                   [4, 128, 3, 4, 3]         --\n",
      "â”‚    â””â”€Flatten: 2-64                     [4, 4608]                 --\n",
      "â”‚    â””â”€Linear: 2-65                      [4, 32]                   147,488\n",
      "â”‚    â””â”€ReLU: 2-66                        [4, 32]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-67                 [4, 32]                   64\n",
      "â”‚    â””â”€Dropout: 2-68                     [4, 32]                   --\n",
      "â”‚    â””â”€Linear: 2-69                      [4, 16]                   528\n",
      "â”‚    â””â”€ReLU: 2-70                        [4, 16]                   --\n",
      "â”‚    â””â”€BatchNorm1d: 2-71                 [4, 16]                   32\n",
      "â”‚    â””â”€Dropout: 2-72                     [4, 16]                   --\n",
      "â”‚    â””â”€Linear: 2-73                      [4, 1]                    17\n",
      "â”‚    â””â”€Sigmoid: 2-74                     [4, 1]                    --\n",
      "==========================================================================================\n",
      "Total params: 3,741,090\n",
      "Trainable params: 3,741,090\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 22.21\n",
      "==========================================================================================\n",
      "Input size (MB): 100.68\n",
      "Forward/backward pass size (MB): 4031.28\n",
      "Params size (MB): 14.96\n",
      "Estimated Total Size (MB): 4146.92\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# get sample\n",
    "sample = data_train[0]\n",
    "# build model\n",
    "model = Net(sample, [8, 16, 32, 64, 128])\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"To cuda\")\n",
    "#     model.cuda()\n",
    "model.summary(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27816173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of the training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1625, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1632, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/tools/data.py\", line 201, in __getitem__\n    participant, session, cohort, _, label = self._get_meta_data(idx)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/clinicadl/tools/deep_learning/data.py\", line 140, in _get_meta_data\n    participant = self.df.loc[image_idx, 'participant_id']\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 889, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1060, in _getitem_tuple\n    return self._getitem_lowerdim(tup)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 807, in _getitem_lowerdim\n    section = self._getitem_axis(key, axis=i)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1124, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1073, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/generic.py\", line 3739, in xs\n    loc = index.get_loc(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3750868908c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mupdate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/train/train_CNN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer_, loader, loss_weights, to_cuda, rescaling)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mto_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3080, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1625, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1632, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 3\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/interpretability-dl-ndd/tools/data.py\", line 201, in __getitem__\n    participant, session, cohort, _, label = self._get_meta_data(idx)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/clinicadl/tools/deep_learning/data.py\", line 140, in _get_meta_data\n    participant = self.df.loc[image_idx, 'participant_id']\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 889, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1060, in _getitem_tuple\n    return self._getitem_lowerdim(tup)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 807, in _getitem_lowerdim\n    section = self._getitem_axis(key, axis=i)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1124, in _getitem_axis\n    return self._get_label(key, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1073, in _get_label\n    return self.obj.xs(label, axis=axis)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/generic.py\", line 3739, in xs\n    loc = index.get_loc(key)\n  File \"/network/lustre/dtlake01/aramis/users/sasha.collin/miniconda3/envs/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 3082, in get_loc\n    raise KeyError(key) from err\nKeyError: 3\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# record losses\n",
    "train_losses = dict()\n",
    "test_losses = dict()\n",
    "\n",
    "# callbacks\n",
    "ES = EarlyStopping(patience=5)\n",
    "MC = ModelCheckpoint()\n",
    "\n",
    "print(\"Beginning of the training\")\n",
    "\n",
    "# training\n",
    "for epoch in range(2):\n",
    "    update_dict(train_losses, train(epoch, model, optimizer, train_loader, to_cuda=False))\n",
    "    update_dict(test_losses, test(model, valid_loader, to_cuda=False, rescaling=stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ff3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,5):\n",
    "    print(np.linalg.norm(getattr(getattr(model,'branch' + str(k)), 'b' + str(k) + '-conv').weight.grad.data.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c3703",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loss visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08832b",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_losses(dict_losses, title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the different losses.\n",
    "    \n",
    "    Args:\n",
    "        dict_losses: dictionnary of losses\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    for key in dict_losses.keys():\n",
    "        plt.plot(dict_losses[key], label=key)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_tensor(X):\n",
    "    x = np.transpose(X[0], (1,2,0))\n",
    "    x = (x-x.min())/x.max()\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb76b5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metric_path = 'results/models/model_5/'\n",
    "\n",
    "train_metrics = pd.read_csv(metric_path + 'train_losses.csv')\n",
    "test_metrics = pd.read_csv(metric_path + 'val_losses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d51b84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_losses(train_metrics[['disease']], \"Training losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f6ecb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_losses(test_metrics[['disease']], \"Val losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cce37e",
   "metadata": {},
   "source": [
    "## Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "saved_data = torch.load('results/models/model_14/best_model.pt')\n",
    "\n",
    "model.load_state_dict(saved_data['model_state_dict'])\n",
    "\n",
    "# select one image\n",
    "img = sample['image'].float()\n",
    "\n",
    "mode = 'cuda'\n",
    "if mode == 'cuda':\n",
    "    model = model.cuda()\n",
    "    img = img.cuda()\n",
    "else:\n",
    "    model = model.cpu()\n",
    "    img = img.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7496f0",
   "metadata": {},
   "source": [
    "### Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb3ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GBP = GuidedBackprop(model)\n",
    "attention_maps = GBP.generate_gradients(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55feea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_explanations(img.cpu(), attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbf4a1",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f2de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a0 = time.time()\n",
    "GC = GradCam(model.cuda())\n",
    "img = sample['image'].float()\n",
    "attentions = GC.get_explanations(img.cuda(), resize=True, to_cpu=True)\n",
    "print(time.time() - a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c11aca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for key in attentions:\n",
    "#     attentions[key] = attentions[key].cpu()\n",
    "visualize_explanations(img, attentions) #, targets='volumes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b69953",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = time.time()\n",
    "GC = GradCam(model.cuda(), target_layer='conv2')\n",
    "img = sample['image'].float()\n",
    "attentions = GC.get_explanations(img.cuda(), resize=True, to_cpu=True)\n",
    "print(time.time() - a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca37236",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_explanations(img, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab791723",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_explanations(img, attentions, targets='disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6006ddc6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### max-sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62552349",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d = [], [], [], []\n",
    "n_max = 500\n",
    "step = 20\n",
    "for k in tqdm(range(1, n_max, step)):\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    resu = max_sensitivity(img, GC, k)\n",
    "    a.append(resu['branch1'])\n",
    "    b.append(resu['branch2'])\n",
    "    c.append(resu['branch3'])\n",
    "    d.append(resu['branch4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca86141",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "max_sensitivity(img, GC, N=1)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed76d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for elem in (a, b, c ,d):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(1,n_max,step), elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7be98",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MoRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d3410",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "attention = attentions['branch1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8e733",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ids = np.unravel_index(np.argsort(-attention, axis=None), attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c76b26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "group_size = 20000\n",
    "batch_X = torch.tile(img[None,...], (batch_size,1,1,1,1))\n",
    "for k in range(1, batch_size):\n",
    "    index = k*group_size\n",
    "    batch_X[k,0,ids[0][:index], ids[1][:index], ids[2][:index]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715326f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model2 = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e5b58",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "resu = model2(batch_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a2c89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = sample['image'].float()\n",
    "np.prod(img.shape)//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde01433",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MoRF(X, model, exp_method, K=None, group_size=20000, AUC=False, batch_size=16, to_cuda=False):\n",
    "    \"\"\"\n",
    "    Most relevant first: measures the reliability of an explanation by testing \n",
    "    how fast the output decreases, while we progressively remove information (e.g., perturb pixels) \n",
    "    from the input ğ‘¥ğ‘¥ (e.g., image), that appears as the most relevant by the explanation.\n",
    "    Args:\n",
    "        X: tensor, brain image, with shape (1, n_channels, depth, height, width). The two first dimensions\n",
    "            are optional.\n",
    "        exp_method: explanation method. Must have a get_explanations(self, input_image) attribute function\n",
    "            which takes an image as input and returns a dictionary mapping branches to explanation maps\n",
    "        K: number of group of relevant pixels to remove\n",
    "        group_size: int, size of a group of pixels to remove\n",
    "        AUC: bool. If True: compute and return area under the curve obtained after removing successively \n",
    "            the K most relevant pixels.\n",
    "        batch_size: int, number of images passed to the model each time\n",
    "            \n",
    "    TO DO:\n",
    "        - add several methods to perturb pixels\n",
    "    \"\"\"\n",
    "    if to_cuda and torch.cuda.is_available():\n",
    "        X = X.cuda()\n",
    "        \n",
    "    if K is None:\n",
    "        K = np.prod(X.shape)//8\n",
    "    \n",
    "    # reshpae X if necessary\n",
    "    while len(X.shape) < 5:\n",
    "        X = X[None,...]\n",
    "    \n",
    "    # original predictions\n",
    "    preds = model(X)\n",
    "    # explanations for original image\n",
    "    expls = exp_method.get_explanations(X, resize=True)\n",
    "    # explanations for new images\n",
    "    new_preds = dict()\n",
    "    \n",
    "    # def update_dict\n",
    "    \n",
    "    for target in expls:\n",
    "        # Indices of the sorted elements of the explanations:\n",
    "        ids = np.unravel_index(np.argsort(-expls[target], axis=None), expls[target].shape)\n",
    "    \n",
    "        if AUC:\n",
    "            # number of \n",
    "            removed_pixels = 0\n",
    "            while removed_pixels < K:\n",
    "                # create batch of images\n",
    "                bs = min(batch_size,(K - removed_pixels)%group_size)\n",
    "                batch_X = torch.tile(X, (bs,1,1,1,1))\n",
    "                for k in range(1, batch_size):\n",
    "                    index = k*group_size\n",
    "                    batch_X[k,0,ids[0][:index], ids[1][:index], ids[2][:index]] = 0\n",
    "                    new_preds[target] = model(batch_X)\n",
    "        else:\n",
    "            # compute MoRF removing the K most relevant pixels\n",
    "            batch_X = X.copy()\n",
    "            batch_X[0,0,ids[0][:K], ids[1][:K], ids[2][:K]]\n",
    "            new_preds[target] = model(batch_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89dc28",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a1bc3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses = pd.read_csv('train_losses_3D_2.csv')\n",
    "test_losses = pd.read_csv('val_losses_3D_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65649095",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f319ae7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(test_losses[['disease', 'volumes', 'age', 'sex']])\n",
    "plt.ylim([0,150])\n",
    "plt.legend(['disease', 'volumes', 'age', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991ce04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[['disease', 'volumes', 'age', 'sex']])\n",
    "plt.legend(['disease', 'volumes', 'age', 'sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09078e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall\n",
    "target = torch.tensor([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "preds = torch.tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
    "metrics = MetricCollection([Accuracy(),\n",
    "                            Precision(num_classes=2, average='micro'),\n",
    "                            Recall(num_classes=3, average='macro')])\n",
    "metrics(preds, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d00a1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# assess gradient norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4813001",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_gradient_norms(model): \n",
    "    path = 'results/models/model_{}/log.out'.format(model)\n",
    "    \n",
    "    file = None\n",
    "    with open(path, 'r') as f:\n",
    "        file = f.read()\n",
    "\n",
    "    data = [ s[36:] for s in file.split('\\n') if 'STDOUT' in s]\n",
    "    data = [ s for k,s in enumerate(data[1:]) if data[k] == 'GRADIENT']\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "def compute_metrics(data, name):\n",
    "    dict_ = {'mean': data.mean(),\n",
    "             'min': data.min(),\n",
    "             'max': data.max(), \n",
    "             'std': data.std(), \n",
    "             'median': np.median(data)}\n",
    "    return pd.DataFrame(dict_, index=[name])\n",
    "\n",
    "def get_all_metrics():\n",
    "    df = None\n",
    "    for k in range(4):\n",
    "        data = get_gradient_norms(9+k)\n",
    "        if df is None:\n",
    "            df = compute_metrics(data,BRANCH2TARGET['branch' + str(k+1)])\n",
    "        else:\n",
    "            df = df.combine_first(compute_metrics(data,BRANCH2TARGET['branch' + str(k+1)]))\n",
    "    return df\n",
    "\n",
    "def plot_training(model):\n",
    "    fig, ax = plt.subplots(1,4,figsize=(16,4))\n",
    "    for k, target in enumerate(TARGET2BRANCH):\n",
    "        df_train = pd.read_csv('results/models/model_{}/train_losses.csv'.format(model))\n",
    "        df_val = pd.read_csv('results/models/model_{}/val_losses.csv'.format(model))\n",
    "        ax[k].plot(getattr(df_train, target), label='train')\n",
    "        ax[k].plot(getattr(df_val, target), label='val')\n",
    "        ax[k].set_title(target)\n",
    "    plt.legend()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d09e46",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = get_all_metrics()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707fc1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights = (1/df['mean']).to_numpy()\n",
    "weights = weights/weights.sum()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4639e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_training(model=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996e6bb",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('results/models/model_{}/val_losses.csv'.format(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9f97d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Age investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f54fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([training_df, valid_df], ignore_index=True)\n",
    "df2 = df_add_data.merge(df, on =['participant_id', 'session_id'], how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c7107",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.title('Age Distribution')\n",
    "plt.hist(training_df.age, label='training')\n",
    "plt.hist(valid_df.age, label='validation')\n",
    "plt.xlabel('age')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99854d6c",
   "metadata": {},
   "source": [
    "# RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab833fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_data[df_add_data.iloc[:,3].isna()].isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cbb3a",
   "metadata": {},
   "source": [
    "I am using working with the file '/network/lustre/dtlake01/aramis/datasets/adni/caps/caps_v2021.tsv' and using the 't1-volume' pipeline and the atlas 'AAL2'.\n",
    "6267 samples (i.e. couples (participant_id, session_id)) do not have any volume value (i.e. NaN). 1 sample has no sex value and 1178 samples have no age values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293be254",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the model\n",
    "convolutions = {\n",
    "    'conv0':{\n",
    "        'n_conv':1,\n",
    "        'in_channels':None,\n",
    "        'out_channels':16,\n",
    "        'd_reduction': \"MaxPooling\"\n",
    "    },\n",
    "    'conv1':{\n",
    "        'n_conv':1,\n",
    "        'in_channels':None,\n",
    "        'out_channels':32,\n",
    "        'd_reduction': \"MaxPooling\"\n",
    "    },\n",
    "    'conv2':{\n",
    "        'n_conv':1,\n",
    "        'in_channels':None,\n",
    "        'out_channels':32,\n",
    "        'd_reduction': \"MaxPooling\"\n",
    "    },\n",
    "    'conv3':{\n",
    "        'n_conv':1,\n",
    "        'in_channels':None,\n",
    "        'out_channels':64,\n",
    "        'd_reduction': \"MaxPooling\"\n",
    "    },\n",
    "    'conv4':{\n",
    "        'n_conv':1,\n",
    "        'in_channels':None,\n",
    "        'out_channels':128,\n",
    "        'd_reduction': \"MaxPooling\"\n",
    "    }\n",
    "}\n",
    "model = RandomArchitecture(convolutions,1,data_train.size)\n",
    "\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = get_criterion(loss)\n",
    "optimizer = getattr(torch.optim, optimizer_name)(filter(lambda x: x.requires_grad, model.parameters()),\n",
    "                                                   lr=learning_rate,\n",
    "                                                   weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ee7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.beginning_epoch = 0\n",
    "        self.tolerance = 0\n",
    "        self.patience = 10\n",
    "        self.epochs = 10\n",
    "        self.gpu = True\n",
    "        self.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb8367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(model, train_loader, valid_loader, criterion, optimizer,False,'log_dir','model_dir', Options())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
